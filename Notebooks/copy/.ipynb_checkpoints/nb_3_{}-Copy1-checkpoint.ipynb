{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine.engine import Engine,Events\n",
    "from ignite.metrics import RunningAverage,Accuracy,Loss\n",
    "from ignite.handlers import ModelCheckpoint,EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from libnlp import preprocessing\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from ignite.utils import convert_tensor\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Jamiu Afolabi/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n",
      "Using cache found in C:\\Users\\Jamiu Afolabi/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "class Env:\n",
    "    def __init__(self,tokenizer_model='bert-base-multilingual-uncased',max_len=120,seed=42):\n",
    "        \n",
    "        #Defining the paths\n",
    "        self.data_path='../Data'\n",
    "        self.train_path='Train.csv'\n",
    "        self.test_path='Test.csv'\n",
    "        self.ppd_path='../ppd'\n",
    "        \n",
    "        #Definining the tokenizer\n",
    "        self.tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model )\n",
    "        self.model=torch.hub.load('huggingface/pytorch-transformers', 'model',tokenizer_model )\n",
    "        self.tokenizer_max_len=max_len\n",
    "        self.cls_token=self.tokenizer.cls_token_id\n",
    "        self.sep_token=self.tokenizer.sep_token_id\n",
    "        self.pad_token=self.tokenizer.pad_token_id\n",
    "        self.unk_token=self.tokenizer.unk_token_id\n",
    "        \n",
    "        #Setting the Seed Value\n",
    "        if torch.cuda.is_available():\n",
    "            self.device='cuda'\n",
    "        else:\n",
    "            self.device='cpu'\n",
    "        torch.manual_seed(seed)\n",
    "        os.environ['PYTHONHASHSEED']=str(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=False\n",
    "E=Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تي',\n",
       " 'بلاد',\n",
       " 'ف',\n",
       " '##اس',\n",
       " '##دة',\n",
       " 'يا',\n",
       " 'ح',\n",
       " '##سون',\n",
       " '##ة',\n",
       " 'ح',\n",
       " '##ظ',\n",
       " '##ك',\n",
       " 'ال',\n",
       " 'واقع',\n",
       " 'ب',\n",
       " 'اد',\n",
       " 'ف',\n",
       " '##اس',\n",
       " '##دة',\n",
       " 'و',\n",
       " 'ر',\n",
       " '##بي',\n",
       " 'ي',\n",
       " '##هن',\n",
       " '##يك',\n",
       " 'في',\n",
       " 'دولة',\n",
       " 'ف',\n",
       " '##اس',\n",
       " '##دة',\n",
       " 'و',\n",
       " 'عالم',\n",
       " 'س',\n",
       " '##اب',\n",
       " '##ع',\n",
       " 'و',\n",
       " 'ما',\n",
       " 'تصل',\n",
       " '##ح',\n",
       " 'ك',\n",
       " '##ن',\n",
       " 'لم',\n",
       " '##ثل',\n",
       " '##ك',\n",
       " 'في',\n",
       " '##ل',\n",
       " 'ع',\n",
       " '##ص',\n",
       " '##ع']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.tokenizer.tokenize('تي بلاد فاسدة يا حسونة حظك إل واقع ب أد فاسدة و ربي يهنيك في دولة فاسدة و عالم سابع و ما تصلح كن لمثلك فيل عصع')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"A\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"H\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"$\": u\"\\u0634\", # shiin\n",
    "            \"S\": u\"\\u0635\", # Saad\n",
    "            \"D\": u\"\\u0636\", # Daad\n",
    "            \"T\": u\"\\u0637\", # Taa'\n",
    "            \"Z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"E\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"F\": u\"\\u064B\", # fatHatayn\n",
    "            \"N\": u\"\\u064C\", # Dammatayn\n",
    "            \"K\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "\n",
    "def transString(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "      if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "      else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'لَ ِلَهَ ِلَ َللَه'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transString('la ilaha ila allah',reverse=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def preprocessed_df(data_path,csv_p,ppd_p,train=True):\n",
    "    p_csv=os.path.join(data_path,csv_p)\n",
    "    df=pd.read_csv(p_csv,encoding='utf-8')\n",
    "    df['text']=df.text.apply(lambda x: preprocessing.preprocess(x))\n",
    "    if train:\n",
    "        ls1=set(df.label)\n",
    "        # {0: 0, 1: 1, -1: 2}\n",
    "        df['label']=df.label.map(dict([(key,val) for val,key in enumerate(ls1)]))\n",
    "    df.drop(['ID'],axis=1,inplace=True)\n",
    "    f_name=csv_p.split('.')[0]\n",
    "    check_dir(ppd_p)\n",
    "    df.to_csv(f'{ppd_p}/{f_name}.csv',index=False)\n",
    "    \n",
    "preprocessed_df(E.data_path,E.train_path,E.ppd_path)\n",
    "preprocessed_df(E.data_path,E.test_path,E.ppd_path,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabiziDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len,train=True):\n",
    "        self.train=train\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "        self.text=df.text.values\n",
    "        if train:\n",
    "            self.labels=df.label.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)  \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        token,mask,len_token=self.token_mask(self.text[idx],self.max_len)\n",
    "        if self.train:\n",
    "            label=self.labels[idx]\n",
    "            return token,mask,len_token,label\n",
    "        return token,mask,len_token\n",
    "    def token_mask(self,text,max_len):\n",
    "        if max_len in range(511,513):\n",
    "            len_text=min(max_len-2,len(text))\n",
    "        else:\n",
    "            len_text=min(max_len,len(text))\n",
    "        text=text[:len_text]\n",
    "        token=self.custTokenizer(self.tokenizer,text)\n",
    "        len_token=len(token)\n",
    "        mask= [1] * len_token\n",
    "        return token,mask,len_token\n",
    "    \n",
    "    def custTokenizer(self,tokenizer,text):\n",
    "        return tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPadding(batch,tokenizer=E.tokenizer):\n",
    "    comp=list(zip(*batch))\n",
    "    tokens=comp[0]\n",
    "    masks=comp[1]\n",
    "    len_tokens=comp[2]      \n",
    "    max_len=max(len_tokens)\n",
    "    tokens_ret=[]\n",
    "    masks_ret=[]\n",
    "    \n",
    "    for idx in range(len(tokens)):\n",
    "        pad_len=max_len-min(len_tokens[idx],max_len)\n",
    "        padding=[tokenizer.pad_token_id] * pad_len\n",
    "        token=tokens[idx] + padding\n",
    "        mask=masks[idx] + [0] * pad_len\n",
    "        tokens_ret.append(token)\n",
    "        masks_ret.append(mask)\n",
    "        \n",
    "    if len(comp)==4:\n",
    "        labels=comp[3]\n",
    "        return torch.tensor(tokens_ret),torch.tensor(masks_ret),torch.tensor(labels)\n",
    "    return torch.tensor(tokens_ret),torch.tensor(masks_ret)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(os.path.join(E.ppd_path,E.train_path),encoding='utf-8')\n",
    "train_df_idx=[i for i,text in enumerate(train_df.text.values) if len(text)>7]\n",
    "train_df=train_df.iloc[train_df_idx]\n",
    "test_df=pd.read_csv(os.path.join(E.ppd_path,E.test_path),encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    37965\n",
       "2    29194\n",
       "0     2410\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "train_ds=ArabiziDataset(train_df,E.tokenizer,E.tokenizer_max_len)\n",
    "test_ds=ArabiziDataset(test_df,E.tokenizer,E.tokenizer_max_len,train=False)\n",
    "train_idx,test_idx=train_test_split(list(range(len(train_ds))),test_size=0.25,random_state=42)\n",
    "train_s,test_s=SubsetRandomSampler(train_idx),SubsetRandomSampler(test_idx)\n",
    "train_loader=DataLoader(train_ds,batch_size=32,collate_fn=customPadding,sampler=train_s)\n",
    "validation_loader=DataLoader(train_ds,batch_size=32,collate_fn=customPadding,sampler=test_s)\n",
    "test_loader=DataLoader(test_ds,batch_size=256,collate_fn=customPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c=next(iter(validation_loader))\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self,n_outputs,bert_model=E.model):\n",
    "        super(BertModel,self).__init__()\n",
    "        self.K=n_outputs\n",
    "        self.bert_model=E.model\n",
    "        self.bert_hidden_size=E.model.config.hidden_size\n",
    "        \n",
    "        self.conv1=nn.Conv1d(self.bert_hidden_size,32,3,padding=1)\n",
    "        self.pool1=nn.MaxPool1d(2)\n",
    "        self.conv2=nn.Conv1d(32,64,3,padding=1)\n",
    "        self.pool2=nn.MaxPool1d(2)\n",
    "        self.conv3=nn.Conv1d(64,128,3,padding=1)\n",
    "        self.pool3=nn.MaxPool1d(2)\n",
    "        self.conv4=nn.Conv1d(128,256,3,padding=1)\n",
    "        \n",
    "        self.fc1=nn.Linear(256,128)\n",
    "        self.fc2=nn.Linear(128,self.K)\n",
    "        \n",
    "    def forward(self,input_tokens,attention_mask):\n",
    "        out,_=self.bert_model(input_ids=input_tokens,attention_mask=attention_mask,return_dict=False)\n",
    "        out=out.permute(0,2,1)\n",
    "        out=self.conv1(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool1(out)\n",
    "        out=self.conv2(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool2(out)\n",
    "        out=self.conv3(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool3(out)\n",
    "        out=self.conv4(out)\n",
    "        out=F.relu(out)\n",
    "        \n",
    "        out=out.permute(0,2,1)\n",
    "        out,_=torch.max(out,1)\n",
    "        out=self.fc1(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        #out=F.log_softmax(out,dim=1)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BertModel(3)\n",
    "model.to(E.device)\n",
    "set_trainable(model.bert_model,False)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building helper to Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch(batch,train=True):\n",
    "    if train:\n",
    "        input_token,attention_mask,targets=batch\n",
    "        return input_token.to(E.device),attention_mask.to(E.device),targets.to(E.device)\n",
    "    else:\n",
    "        input_token,attention_mask=batch\n",
    "        return input_token.to(E.device),attention_mask.to(E.device)\n",
    "\n",
    "def trainer_update(engine,batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    input_token,attention_mask,y=_batch(batch)\n",
    "    #Forward pass\n",
    "    out=model(input_token,attention_mask)\n",
    "    loss=criterion(out,y)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print('Done')\n",
    "    return loss.item()\n",
    "\n",
    "def eval_update(engine,batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_token,attention_mask,y=_batch(batch)\n",
    "        out=model(input_token,attention_mask)\n",
    "        y_pred=torch.round(out)\n",
    "        return y_pred,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Engine(trainer_update)\n",
    "train_evaluator=Engine(eval_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')\n",
    "Loss(criterion).attach(train_evaluator,'crossentropy')\n",
    "Accuracy().attach(train_evaluator,'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar=ProgressBar(persist=True,bar_format=\"\")\n",
    "pbar.attach(trainer,['loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x278375fb940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_func(engine):\n",
    "    val_loss=engine.state.metrics['crossentropy']\n",
    "    return -val_loss\n",
    "\n",
    "handler=EarlyStopping(patience=5,score_function=score_func,trainer=trainer)\n",
    "train_evaluator.add_event_handler(Events.COMPLETED,handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x27815291730>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_res():\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics=train_evaluator.state.metrics\n",
    "    accuracy=metrics['accuracy']\n",
    "    loss=metrics['crossentropy']\n",
    "    pbar.log_message(f'Training_loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_res():\n",
    "    train_evaluator.run(validation_loader)\n",
    "    metrics=train_evaluator.state.metrics\n",
    "    accuracy=metrics['accuracy']\n",
    "    loss=metrics['crossentropy']\n",
    "    pbar.log_message(f'Validation_loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "checkpoint=ModelCheckpoint('/tmp/modeltest','bert_model',n_saved=3,require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED,checkpoint,{'bertmodel':model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6713ef0de55b488c960709ac633450f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1631.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training_loss: 0.5895265768145316, Accuracy: 0.7281700398650721\n",
      "Validation_loss: 0.6054556677794227, Accuracy: 0.7210371988731099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 1631\n",
       "\tepoch: 1\n",
       "\tepoch_length: 1631\n",
       "\tmax_epochs: 1\n",
       "\toutput: 0.711725115776062\n",
       "\tbatch: <class 'tuple'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_loader,max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        input_token,attention_mask=_batch(batch,train=False)\n",
    "        out=model(input_token,attention_mask)\n",
    "        predicted = torch.max(out,1)[1]\n",
    "        y=predicted.cpu().detach().numpy()\n",
    "        predictions.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.hstack(predictions)\n",
    "predictions=np.transpose(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=pd.read_csv(os.path.join(E.data_path,'SampleSubmission.csv'))\n",
    "test=pd.read_csv(os.path.join(E.data_path,'Test.csv'))\n",
    "ss.ID=test.ID\n",
    "ss.label=predictions\n",
    "mapping={0: 0, 1: 1, -1: 2}\n",
    "inverse_mapping={x:y for y,x in mapping.items()}\n",
    "ss['label']=ss.label.map(inverse_mapping)\n",
    "ss.label.value_counts()\n",
    "ss.to_csv('../Data/nb_3_ss_{}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    15488\n",
       " 1    14512\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
