{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Jamiu Afolabi/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from libnlp import preprocessing\n",
    "import pandas as pd\n",
    "import torchtext.data as ttd\n",
    "import torch\n",
    "\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "class Env:\n",
    "    def __init__(self,tokenizer_model='bert-base-multilingual-uncased',max_len=120,seed=42):\n",
    "        \n",
    "        #Defining the paths\n",
    "        self.data_path='../Data'\n",
    "        self.train_path='Train.csv'\n",
    "        self.test_path='Test.csv'\n",
    "        self.ppd_path='../ppd'\n",
    "        \n",
    "        #Definining the tokenizer\n",
    "        self.tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model )\n",
    "        self.tokenizer_max_len=max_len\n",
    "        self.cls_token=self.tokenizer.cls_token_id\n",
    "        self.sep_token=self.tokenizer.sep_token_id\n",
    "        self.pad_token=self.tokenizer.pad_token_id\n",
    "        self.unk_token=self.tokenizer.unk_token_id\n",
    "        \n",
    "        #Setting the Seed Value\n",
    "        if torch.cuda.is_available():\n",
    "            self.device='cuda'\n",
    "        else:\n",
    "            self.device='cpu'\n",
    "        torch.manual_seed(seed)\n",
    "        os.environ['PYTHONHASHSEED']=str(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=False\n",
    "E=Env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def preprocessed_df(data_path,csv_p,ppd_p,train=True):\n",
    "    p_csv=os.path.join(data_path,csv_p)\n",
    "    df=pd.read_csv(p_csv,encoding='utf-8')\n",
    "    df['text']=df.text.apply(lambda x: preprocessing.preprocess(x))\n",
    "    if train:\n",
    "        ls1=set(df.label)\n",
    "        # {0: 0, 1: 1, -1: 2}\n",
    "        df['label']=df.label.map(dict([(key,val) for val,key in enumerate(ls1)]))\n",
    "    df.drop(['ID'],axis=1,inplace=True)\n",
    "    f_name=csv_p.split('.')[0]\n",
    "    check_dir(ppd_p)\n",
    "    df.to_csv(f'{ppd_p}/{f_name}.csv',index=False)\n",
    "    \n",
    "preprocessed_df(E.data_path,E.train_path,E.ppd_path)\n",
    "preprocessed_df(E.data_path,E.test_path,E.ppd_path,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.tokenizer_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_max(tweet):\n",
    "    \n",
    "    if len(tweet)>E.tokenizer_max_len-2:\n",
    "        #print('greater')\n",
    "        tweet=tweet[:E.tokenizer_max_len-2]\n",
    "        #print('in token')\n",
    "    tokens=E.tokenizer.tokenize(tweet,add_special_tokens=False,padding=True)\n",
    "    #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "class DataPrep:\n",
    "    def __init__(self):\n",
    "    #creating the field object\n",
    "        self.TEXT=ttd.Field(batch_first=True,\n",
    "                            use_vocab=False,\n",
    "                            pad_first=True,\n",
    "                            tokenize=tokenize_max,\n",
    "                            preprocessing=E.tokenizer.convert_tokens_to_ids,\n",
    "                            init_token=E.cls_token,\n",
    "                            eos_token=E.sep_token,\n",
    "                            pad_token=E.pad_token,\n",
    "                            unk_token=E.unk_token,\n",
    "                            )\n",
    "        \n",
    "        self.LABEL=ttd.Field(sequential=False,\n",
    "                             is_target=True,\n",
    "                             use_vocab=False) \n",
    "        self.dataset=ttd.TabularDataset(path='../ppd/Train.csv',\n",
    "                                        format='csv',\n",
    "                                        skip_header=True,\n",
    "                                        fields=[('text',self.TEXT),('label',self.LABEL)])\n",
    "        \n",
    "        self.train_data,self.test_data=self.dataset.split()\n",
    "        self.train_iter,self.test_iter=ttd.Iterator.splits((self.train_data,self.test_data),\n",
    "                                                           sort_key=lambda x: len(x.text),\n",
    "                                                           batch_sizes=(32,32),\n",
    "                                                           device=E.device\n",
    "                                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=DataPrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 5])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for inputs,target in D.test_iter:\n",
    "    i+=1\n",
    "    print(inputs.shape)\n",
    "    print(target.shape)\n",
    "    if i==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,n_vocabs,n_embedding,n_hidden,n_layer,n_output):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        self.V=n_vocabs\n",
    "        self.D=n_embedding\n",
    "        self.M=n_hidden\n",
    "        self.L=n_layer\n",
    "        self.K=n_output\n",
    "        \n",
    "        self.embed=nn.Embedding(self.V,self.D)\n",
    "        self.rnn=nn.LSTM(input_size=self.D,\n",
    "                         hidden_size=self.M,\n",
    "                         num_layers=self.L,\n",
    "                         batch_first=True,\n",
    "                         bidirectional=False\n",
    "                        )\n",
    "        self.fc=nn.Linear(self.M,1024)\n",
    "        self.fc2=nn.Linear(1024,128)\n",
    "        self.fc3=nn.Linear(128,self.K)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        bs=X.size(0)\n",
    "        #Defining parameters for rnn instantiation\n",
    "        h0=torch.zeros(self.L,X.size(0),self.M).to(E.device)\n",
    "        c0=torch.zeros(self.L,X.size(0),self.M).to(E.device)\n",
    "        \n",
    "        out=self.embed(X)\n",
    "        out,_=self.rnn(out,(h0,c0))\n",
    "        out,_=torch.max(out,1)\n",
    "        out=out.view(bs,-1)\n",
    "        out=self.fc(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.fc3\n",
    "        out\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (embed): Embedding(105879, 64)\n",
       "  (rnn): LSTM(64, 50, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'n_vocabs':E.tokenizer.vocab_size,\n",
    "        'n_embedding':64,\n",
    "        'n_hidden':50,\n",
    "        'n_layer':2,\n",
    "        'n_output':3\n",
    "       }\n",
    "model=SimpleRNN(**params)\n",
    "model.to(E.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {}
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining Loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs=10\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0=datetime.now()\n",
    "    train_loss=[]\n",
    "    for inputs,targets in D.train_iter:\n",
    "        #targets=targets.view(-1,1)\n",
    "        \n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        out=model(inputs)\n",
    "        loss=criterion(out,targets)\n",
    "        #back\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    train_loss=np.mean(train_loss)\n",
    "    \n",
    "    test_loss=[]\n",
    "    \n",
    "    for inputs,targets in D.test_iter:\n",
    "        \n",
    "        #targets=targets.view(-1,1).float()\n",
    "        out=model(inputs)\n",
    "        loss=criterion(out,targets)\n",
    "        test_loss.append(loss.item())\n",
    "    test_loss=np.mean(test_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    t1=datetime.now()\n",
    "    duration=t1-t0\n",
    "    print(f'{epoch+1}/{n_epochs}, train_loss: {train_loss}, test_loss: {test_loss}, duration: {duration}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
