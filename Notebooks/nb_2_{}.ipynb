{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from libnlp import preprocessing\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from ignite.utils import convert_tensor\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Jamiu Afolabi/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n",
      "Using cache found in C:\\Users\\Jamiu Afolabi/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "class Env:\n",
    "    def __init__(self,tokenizer_model='bert-base-multilingual-uncased',max_len=120,seed=42):\n",
    "        \n",
    "        #Defining the paths\n",
    "        self.data_path='../Data'\n",
    "        self.train_path='Train.csv'\n",
    "        self.test_path='Test.csv'\n",
    "        self.ppd_path='../ppd'\n",
    "        \n",
    "        #Definining the tokenizer\n",
    "        self.tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model )\n",
    "        self.model=torch.hub.load('huggingface/pytorch-transformers', 'model',tokenizer_model )\n",
    "        self.tokenizer_max_len=max_len\n",
    "        self.cls_token=self.tokenizer.cls_token_id\n",
    "        self.sep_token=self.tokenizer.sep_token_id\n",
    "        self.pad_token=self.tokenizer.pad_token_id\n",
    "        self.unk_token=self.tokenizer.unk_token_id\n",
    "        \n",
    "        #Setting the Seed Value\n",
    "        if torch.cuda.is_available():\n",
    "            self.device='cuda'\n",
    "        else:\n",
    "            self.device='cpu'\n",
    "        torch.manual_seed(seed)\n",
    "        os.environ['PYTHONHASHSEED']=str(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=False\n",
    "E=Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.model(return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabiziDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len,train=True):\n",
    "        self.train=train\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "        self.text=df.text.values\n",
    "        self.labels=df.label.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)  \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        token,mask,len_token=self.token_mask(self.text[idx],self.max_len)\n",
    "        if self.train:\n",
    "            label=self.labels[idx]\n",
    "            return token,mask,len_token,label\n",
    "        return token,mask,None\n",
    "    def token_mask(self,text,max_len):\n",
    "        if max_len in range(511,513):\n",
    "            len_text=min(max_len-2,len(text))\n",
    "        else:\n",
    "            len_text=min(max_len,len(text))\n",
    "        text=text[:len_text]\n",
    "        token=self.custTokenizer(self.tokenizer,text)\n",
    "        len_token=len(token)\n",
    "        mask= [1] * len_token\n",
    "        return token,mask,len_token\n",
    "    \n",
    "    def custTokenizer(self,tokenizer,text):\n",
    "        return tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customPadding(batch,tokenizer=E.tokenizer):\n",
    "    comp=list(zip(*batch))\n",
    "    tokens=comp[0]\n",
    "    masks=comp[1]\n",
    "    len_tokens=comp[2]\n",
    "    labels=comp[3]\n",
    "    max_len=max(len_tokens)\n",
    "    tokens_ret=[]\n",
    "    masks_ret=[]\n",
    "    \n",
    "    for idx in range(len(tokens)):\n",
    "        pad_len=max_len-min(len_tokens[idx],max_len)\n",
    "        padding=[tokenizer.pad_token_id] * pad_len\n",
    "        token=tokens[idx] + padding\n",
    "        mask=masks[idx] + [0] * pad_len\n",
    "        tokens_ret.append(token)\n",
    "        masks_ret.append(mask)\n",
    "        \n",
    "    if len(comp)==4:\n",
    "        return [torch.tensor(tokens_ret),torch.tensor(masks_ret)],torch.tensor(labels)\n",
    "    return torch.tensor(tokens_ret),torch.tensor(masks_ret)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(os.path.join(E.ppd_path,E.train_path),encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "ds=ArabiziDataset(train_df,E.tokenizer,E.tokenizer_max_len)\n",
    "train_idx,test_idx=train_test_split(list(range(len(ds))),test_size=0.25)\n",
    "train_s,test_s=SubsetRandomSampler(train_idx),SubsetRandomSampler(test_idx)\n",
    "train_loader=DataLoader(train_ds,batch_size=10,collate_fn=customPadding,sampler=train_s)\n",
    "test_loader=DataLoader(train_ds,batch_size=10,collate_fn=customPadding,sampler=test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter=iter(train_loader)\n",
    "a,b=dataiter.next()\n",
    "e,f=dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,   124, 39359, 36671, 18320, 10167, 10354, 60581,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 10193, 13685,   162, 11444, 20651,   162, 10959, 14461, 10209,\n",
       "          10650, 84147, 10243, 15041, 10546, 10959, 61581, 27320, 53777,   152,\n",
       "          10351, 10163, 28056, 10159, 11518, 10112, 10117, 15167, 10115, 71224,\n",
       "          10959, 10745, 37547, 10112, 17509, 12981, 12438, 17368, 31588, 11378,\n",
       "            102,     0,     0,     0],\n",
       "         [  101, 25410, 34238, 10380, 10593, 23550, 19601, 10116, 10593, 27102,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 11338,   124, 10408, 13533,   161, 10959, 45713, 10709,   165,\n",
       "          41796, 21428, 10218, 32726, 10959, 11180, 11180, 13533, 43310, 24398,\n",
       "          11301, 11068, 83825, 10206, 41548, 10135, 10481, 11301, 11055, 13533,\n",
       "            126, 26218, 10311,   165, 12271, 34238, 10546, 10650, 38075, 29263,\n",
       "          20448, 10112,   102,     0],\n",
       "         [  101, 10238, 10481, 64265, 71224, 10959, 10514, 95630,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 83717, 29412, 11518, 17675, 10136, 10835, 13077, 64080, 10177,\n",
       "          12283, 11719,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 12314, 10959, 11296, 10546, 11444, 13284, 10123, 10972, 23076,\n",
       "            160,   161,   150, 10391, 10283, 10167, 10425, 10145, 11587, 10959,\n",
       "          10952, 12979, 20159, 10167,   143,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 11742, 11880,   167, 81177, 10167,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 22084, 11695, 10593, 10193, 12620, 28230, 22084, 11695,   124,\n",
       "          77630,   124, 80581, 11055, 10117, 41796, 22046, 10136, 19716, 10835,\n",
       "          47945, 10116, 69618, 16497, 20651, 22016, 13533, 10243, 10608, 10126,\n",
       "          10837, 55019, 19280, 10953, 27666, 10116, 10135, 11518, 11156, 10130,\n",
       "          11522, 10163, 17272,   102],\n",
       "         [  101, 33727, 12854, 80536,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self,n_outputs,bert_model=E.model):\n",
    "        super(BertModel,self).__init__()\n",
    "        self.K=n_outputs\n",
    "        self.bert_model=E.model\n",
    "        self.bert_hidden_size=E.model.config.hidden_size\n",
    "        \n",
    "        self.conv1=nn.Conv1d(self.bert_hidden_size,32,3,padding=1)\n",
    "        self.pool1=nn.MaxPool1d(2)\n",
    "        self.conv2=nn.Conv1d(32,64,3,padding=1)\n",
    "        self.pool2=nn.MaxPool1d(2)\n",
    "        self.conv3=nn.Conv1d(64,128,3,padding=1)\n",
    "        self.pool3=nn.MaxPool1d(2)\n",
    "        self.conv4=nn.Conv1d(128,256,3,padding=1)\n",
    "        \n",
    "        self.fc1=nn.Linear(256,128)\n",
    "        self.fc2=nn.Linear(128,self.K)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        out,_=self.bert_model(input_ids=X[0],attention_mask=X[1],return_dict=False)\n",
    "        out=out.permute(0,2,1)\n",
    "        out=self.conv1(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool1(out)\n",
    "        out=self.conv2(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool2(out)\n",
    "        out=self.conv3(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.pool3(out)\n",
    "        out=self.conv4(out)\n",
    "        out=F.relu(out)\n",
    "        \n",
    "        out=out.permute(0,2,1)\n",
    "        out,_=torch.max(out,1)\n",
    "        out=self.fc1(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BertModel(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(E.device)\n",
    "set_trainable(model.bert_model, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train_loss: 0.6363844664934135, test_loss: 0.5904938032541956, duration: 0:05:40.430662\n",
      "2/10, train_loss: 0.5654653862814109, test_loss: 0.5654670892357826, duration: 0:05:40.626023\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining Loss and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs=10\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0=datetime.now()\n",
    "    train_loss=[]\n",
    "    for inputs,targets in train_loader:\n",
    "        inputs,targets=convert_tensor(inputs,device=E.device),convert_tensor(targets,device=E.device)\n",
    "        #targets=targets.view(-1,1)\n",
    "        \n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        out=model(inputs)\n",
    "        loss=criterion(out,targets)\n",
    "        #back\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    train_loss=np.mean(train_loss)\n",
    "    \n",
    "    test_loss=[]\n",
    "    \n",
    "    for inputs,targets in test_loader:\n",
    "        \n",
    "        #targets=targets.view(-1,1).float()\n",
    "        inputs,targets=convert_tensor(inputs,device=E.device),convert_tensor(targets,device=E.device)\n",
    "        out=model(inputs)\n",
    "        loss=criterion(out,targets)\n",
    "        test_loss.append(loss.item())\n",
    "    test_loss=np.mean(test_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    t1=datetime.now()\n",
    "    duration=t1-t0\n",
    "    print(f'{epoch+1}/{n_epochs}, train_loss: {train_loss}, test_loss: {test_loss}, duration: {duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
